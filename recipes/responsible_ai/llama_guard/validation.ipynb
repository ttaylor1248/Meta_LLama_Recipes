{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c29b97c-1f79-4f26-a019-a1f85a85b9c6",
   "metadata": {},
   "source": [
    "# Validating the output of Llama Guard quantized and unquantized\n",
    "\n",
    "This notebook aims to show how to validate Llama Guard performance on a given dataset. The script loads a given dataset and evaluates each prompt individually against Llama Guard. To evaluate performance, we calculate the averate precision of the binary classification for a given prompt. Llama Guard can be run usgin Meta provided weights or directly from Hugging Face. \n",
    "\n",
    "## Dataset format\n",
    "The dataset should be in a `jsonl` file, with an object per line, following this structure:\n",
    "```\n",
    "{\n",
    "    \"prompt\": \"user_input\",\n",
    "    \"generation\": \"model_response\",\n",
    "    \"label\": \"good/bad\", \n",
    "    \"unsafe_content\": [\"O1\"]\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "The `label` has a `good` or `bad` value to define if the content is considered safe or unsafe, respectively.\n",
    "\n",
    "The `unsafe_content` field contains a list of the categories the prompt is violating.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a33bcb2-1c42-4abb-8e4f-fe5df88fca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16d266b2-c232-4c7e-8beb-624a4719d9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import llm_eval, pytorch_llm_eval, AgentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f361b27c-8601-48e6-b453-14ed051379c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Type(Enum):\n",
    "    HF = \"HF\"\n",
    "    PYTORCH = \"PYTORCH\"\n",
    "\n",
    "def format_prompt(entry, agent_type: AgentType):\n",
    "    prompts = []\n",
    "    if agent_type == AgentType.USER:\n",
    "        prompts = [entry[\"prompt\"]]\n",
    "    else:\n",
    "        prompts = [entry[\"prompt\"], entry[\"generation\"]]\n",
    "\n",
    "    return {\n",
    "        \"prompt\": prompts,\n",
    "        \"agent_type\": agent_type,\n",
    "        \"label\": entry[\"label\"],\n",
    "        \"unsafe_content\": entry[\"unsafe_content\"],\n",
    "        \"idx\": entry[\"idx\"]\n",
    "    }\n",
    "\n",
    "def validate_agent_type(value):\n",
    "    try:\n",
    "        return AgentType(value)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"Invalid AgentType. Choose from: {[agent_type.value for agent_type in AgentType]}\")\n",
    "\n",
    "\n",
    "\n",
    "def run_validation(jsonl_file_path, agent_type, type: Type, load_in_8bit: bool = True, load_in_4bit: bool = False, ckpt_dir = None):\n",
    "\n",
    "    input_file_path = Path(jsonl_file_path)\n",
    "\n",
    "    agent_type = validate_agent_type(agent_type)\n",
    "    \n",
    "    # Preparing prompts\n",
    "    prompts: List[Tuple[List[str], AgentType, str, str, str]] = []\n",
    "    with open(jsonl_file_path, \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            entry = json.loads(line)\n",
    "            \n",
    "            # Format prompt and add to list\n",
    "            prompt = format_prompt(entry, agent_type)\n",
    "            prompts.append(prompt)\n",
    "\n",
    "    \n",
    "    # Executing evaluation\n",
    "    start = time.time()\n",
    "    if type is Type.HF:\n",
    "        llm_eval(prompts, load_in_8bit=load_in_8bit, load_in_4bit=True, logprobs=True)\n",
    "    else:\n",
    "        pytorch_llm_eval(prompts, ckpt_dir, True)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f\"evaluation executed in {end - start} seconds\")\n",
    "        \n",
    "    average_precision = parse_logprobs(prompts, type)\n",
    "    print(f\"average precision {average_precision:.2%}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9562e5-6580-491c-a418-0d8fe9771cf7",
   "metadata": {},
   "source": [
    "## Average presicion\n",
    "\n",
    "This function calculates the average precision score for a set of prompts based on their log probabilities and labels. \n",
    "\n",
    "The `prompts` contain the logprobs calculated for each result by Llama Guard when evaluating the prompts or prompt and generation. \n",
    "\n",
    "The `type` is used to identify if the logprobs are comming from a Hugging Face model or plain pytorch model.\n",
    "\n",
    "The logprob is converted back into probability by exponentiating it (`np.exp`)\n",
    "\n",
    "The probability for `unsafe` when the result is `safe` is calculated using the heuristic 1 - `safe`. As this is a banary classification problem, it should be close to the real value for `unsafe`.\n",
    "\n",
    "The average presicion is calculated with the binary labels from the expected value for each prompt or prompt/generation pair and the probability of generating the unsafe token for each.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04d0097-951f-477f-8493-48008cc5c306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_logprobs(prompts, type: Type):\n",
    "    positive_class_probs = []\n",
    "    for prompt in prompts:\n",
    "        prob = np.exp(prompt[\"logprobs\"][0]) if type is Type.PYTORCH else np.exp(prompt[\"logprobs\"][0][1])\n",
    "        if \"unsafe\" in prompt[\"result\"]:\n",
    "            positive_class_probs.append(prob)\n",
    "        else:\n",
    "            # Using heuristic 1 - `safe` probability to calculate the probability of a non selected token in a binary classification\n",
    "            positive_class_probs.append(1 - prob)\n",
    "        \n",
    "    binary_labels = [1 if prompt[\"label\"] == \"bad\" else 0 for prompt in prompts]\n",
    "\n",
    "    return average_precision_score(binary_labels, positive_class_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57483ddd-f3f0-419f-89e5-53fe6d584c8a",
   "metadata": {},
   "source": [
    "**Note:** If you get a `Address already in use` error when running with a local llama guard model, change the port by setting the environment variable to a new one. e.g.: `os.environ[\"MASTER_PORT\"] = \"29501\"` For more details, check `Inference.ipynb`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b660e42c-ac80-4538-bb55-d0fb097949e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_file = \"prompts.jsonl\"\n",
    "\n",
    "# When the type is pytorch, there is no quantization options\n",
    "run_validation(prompts_file, AgentType.USER, Type.PYTORCH, ckpt_dir = \"path/to/llama_guard/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e31c6df-d0c2-417f-be48-a5532c79b7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up the cache from running the previous validation\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41c1c1f-ca75-4353-8292-852acc73153e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HF to access the model, if necessary\n",
    "# from huggingface_hub import login\n",
    "# login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef07df70-f6bc-459a-9b3d-4cb3bf698757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, load_in_8bit is true. To run unquantized or with 4bit quantization, set load_in_8bit to False and load_in_4bit to true\n",
    "run_validation(prompts_file, AgentType.USER, Type.HF, load_in_8bit = False, load_in_4bit = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cd40ec-57dc-4ac8-aa38-3d274c4e0b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,auto:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
